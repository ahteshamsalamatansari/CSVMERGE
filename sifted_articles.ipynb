{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNRVleWOpnrPr+8XFF/uyYX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahteshamsalamatansari/CSVMERGE/blob/main/sifted_articles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6qhdN883eco"
      },
      "outputs": [],
      "source": [
        "# üï∑Ô∏è Sifted.eu Article Scraper - Colab, FULL SMART VERSION\n",
        "!pip install -q selenium undetected-chromedriver cloudscraper beautifulsoup4\n",
        "!pip install -q pandas matplotlib ipywidgets tqdm fake-useragent nltk\n",
        "\n",
        "import os, re, time, random, requests, cloudscraper, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium import webdriver\n",
        "from google.colab import files\n",
        "from tqdm.notebook import tqdm\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "class TextCleaner:\n",
        "    def __init__(self):\n",
        "        self.social = [r'https?://(?:www\\.)?(?:facebook|fb|twitter|instagram|linkedin|youtube|tiktok|snapchat|pinterest)\\.com/\\S+', r'@\\w+', r'#\\w+']\n",
        "        self.subscr = [\n",
        "            r'subscribe\\s+(?:to|now|here)?', r'sign\\s+up\\s+(?:for|to)?', r'get\\s+(?:our|the)?\\s*newsletter',\n",
        "            r'join\\s+(?:our|the)?\\s*community', r'follow\\s+us\\s+on', r'connect\\s+with\\s+us', r'stay\\s+updated',\n",
        "            r\"don't\\s+miss\\s+out\", r'be\\s+the\\s+first\\s+to\\s+know', r'premium\\s+(?:content|access|subscription)',\n",
        "            r'upgrade\\s+(?:to|your)\\s+(?:account|plan)', r'unlock\\s+(?:full|premium)\\s+(?:content|access)',\n",
        "            r'become\\s+a\\s+(?:member|subscriber)', r'limited\\s+(?:time|access)', r'free\\s+trial', r'pay\\s*wall', r'subscription\\s+required'\n",
        "        ]\n",
        "        self.figure = [\n",
        "            r'figure\\s+\\d+', r'fig\\.\\s*\\d+', r'image\\s+\\d+', r'chart\\s+\\d+', r'graph\\s+\\d+', r'source:\\s*[^\\n]+',\n",
        "            r'credit:\\s*[^\\n]+', r'photo\\s+(?:by|credit):?\\s*[^\\n]+', r'image\\s+(?:by|credit):?\\s*[^\\n]+',\n",
        "            r'getty\\s+images?', r'shutterstock', r'unsplash', r'reuters', r'ap\\s+photo', r'bloomberg'\n",
        "        ]\n",
        "    def clean_text(self, text):\n",
        "        if not text: return \"\"\n",
        "        text = re.sub(r'<[^>]+>', '', text)\n",
        "        for pattern in self.social + self.subscr + self.figure:\n",
        "            text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
        "        text = re.sub(r'https?://\\S+', '', text)\n",
        "        text = re.sub(r'www\\.\\S+', '', text)\n",
        "        text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        nav_words = ['home', 'about', 'contact', 'privacy', 'terms', 'menu', 'search', 'login', 'register', 'logout', 'profile', 'settings', 'help']\n",
        "        for word in nav_words: text = re.sub(r'\\b' + word + r'\\b', '', text, flags=re.IGNORECASE)\n",
        "        return text.strip()\n",
        "\n",
        "class SiftedScraper:\n",
        "    def __init__(self):\n",
        "        try:\n",
        "            opts = Options()\n",
        "            opts.add_argument('--headless'), opts.add_argument('--no-sandbox'), opts.add_argument('--disable-dev-shm-usage')\n",
        "            opts.add_argument('--disable-blink-features=AutomationControlled')\n",
        "            self.driver = webdriver.Chrome(options=opts)\n",
        "        except Exception as e: self.driver = None\n",
        "        self.cloudscraper_session = cloudscraper.create_scraper()\n",
        "        self.cleaner = TextCleaner()\n",
        "    def extract_with_cloudscraper(self, url):\n",
        "        try:\n",
        "            html = self.cloudscraper_session.get(url, timeout=30).text\n",
        "            return self.parse_article(html, url)\n",
        "        except: return None\n",
        "    def extract_with_selenium(self, url):\n",
        "        try:\n",
        "            if not self.driver: return None\n",
        "            self.driver.get(url)\n",
        "            time.sleep(random.uniform(4,6))\n",
        "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
        "            return self.parse_article(str(soup), url)\n",
        "        except: return None\n",
        "\n",
        "    def parse_article(self, html, url):\n",
        "        soup = BeautifulSoup(html, 'html.parser')\n",
        "        title_elem = soup.select_one('h1, h1.title, .article-title, [data-testid=\"headline\"]')\n",
        "        title = self.cleaner.clean_text(title_elem.text) if title_elem else \"\"\n",
        "        # --- Extract only the main narrative, skip all meta/pro/garbage lines ---\n",
        "        container = soup.select_one('article, .article-content, .post-content, main, [class*=content]')\n",
        "        if not container:\n",
        "            paragraphs = soup.find_all('p')\n",
        "        else:\n",
        "            paragraphs = container.find_all('p')\n",
        "        # Clean all <p> blocks\n",
        "        full_paragraphs = [self.cleaner.clean_text(p.get_text()) for p in paragraphs if p.get_text(strip=True)]\n",
        "        header_words = [\"Analysis\", \"Pro\", \"Recommended\", \"minute read\", \"By \", \"Updated\", \"# \", \"Read more\", \"Sifted\", \"Kai Nicol-Schwarz\"]\n",
        "        # Remove header/meta lines & find real start (\"Very few...\", \"The ...\", \"In ...\")\n",
        "        body_candidates = [p for p in full_paragraphs if len(p)>40 and not any(w in p for w in header_words)]\n",
        "        story_start = 0\n",
        "        for i, para in enumerate(body_candidates):\n",
        "            # Adjust to your language/case needs for catching good article starters\n",
        "            if re.match(r'([A-Z][a-z ,\\'\\-]+){3,}', para) or re.match(r'Very few companies', para):\n",
        "                story_start = i; break\n",
        "        content = ' '.join(body_candidates[story_start:]).strip()\n",
        "        author_elem = soup.select_one('.author, .byline, [class*=author]')\n",
        "        author = self.cleaner.clean_text(author_elem.text) if author_elem else \"\"\n",
        "        date_elem = soup.select_one('time, .date, .published, [datetime]')\n",
        "        date = date_elem.get('datetime') if date_elem and date_elem.has_attr('datetime') else (date_elem.text if date_elem else \"\")\n",
        "        return {\"url\": url, \"title\": title, \"content\": content, \"author\": author, \"date\": date}\n",
        "\n",
        "    def scrape(self, urls):\n",
        "        results = []\n",
        "        for url in tqdm(urls, desc=\"Scraping articles\"):\n",
        "            d = self.extract_with_cloudscraper(url) or self.extract_with_selenium(url)\n",
        "            if d and d[\"content\"]:\n",
        "                d[\"word_count\"] = len(d[\"content\"].split())\n",
        "                d[\"reading_time\"] = max(1, d[\"word_count\"]//200)\n",
        "                results.append(d)\n",
        "        if self.driver: self.driver.quit()\n",
        "        return results\n",
        "\n",
        "def run_interface():\n",
        "    file_upload = widgets.FileUpload(accept='.txt,.csv', multiple=False)\n",
        "    url_box = widgets.Textarea(value='', placeholder='Paste URLs here (one per line)', layout=widgets.Layout(width='100%', height='90px'))\n",
        "    run_btn = widgets.Button(description=\"üöÄ Start Scraping\", button_style='success')\n",
        "    out = widgets.Output(); status = widgets.HTML()\n",
        "\n",
        "    def on_start(b):\n",
        "        with out:\n",
        "            clear_output()\n",
        "            lines = []\n",
        "            if file_upload.value:\n",
        "                content = list(file_upload.value.values())[0]['content'].decode('utf-8')\n",
        "                lines = content.strip().splitlines()\n",
        "            elif url_box.value:\n",
        "                lines = url_box.value.strip().splitlines()\n",
        "            urls = []\n",
        "            for line in lines:\n",
        "                m = re.search(r'https?://[^\\s\\)\\]]+', line)\n",
        "                if m: urls.append(m.group())\n",
        "            if not urls:\n",
        "                print(\"‚ùå No URLs found.\"); return\n",
        "            status.value = f\"<b>Scraping {len(urls)} articles...</b>\"\n",
        "            scraper = SiftedScraper()\n",
        "            data = scraper.scrape(urls)\n",
        "            if not data:\n",
        "                print(\"‚ùå No articles scraped.\"); return\n",
        "            df = pd.DataFrame(data)[[\"url\", \"title\", \"content\", \"author\", \"date\", \"word_count\", \"reading_time\"]]\n",
        "            df.to_csv('sifted_articles.csv', index=False)\n",
        "            files.download('sifted_articles.csv')\n",
        "            print(\"\\n‚úÖ Scraping done! CSV downloaded.\\n\")\n",
        "            try:\n",
        "                plt.figure(figsize=(7,4))\n",
        "                df.word_count.plot(kind='bar')\n",
        "                plt.title(\"Article Word Counts\")\n",
        "                plt.ylabel(\"Words\"); plt.xlabel(\"Article\")\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "            except: pass\n",
        "\n",
        "    def check_action(change):\n",
        "        # auto click run_btn if upload or URL entry is non-empty\n",
        "        if (file_upload.value or url_box.value.strip()):\n",
        "            run_btn.click()\n",
        "    file_upload.observe(check_action, names='value')\n",
        "    url_box.observe(check_action, names='value')\n",
        "    run_btn.on_click(on_start)\n",
        "    ui = widgets.VBox([\n",
        "        widgets.HTML(\"<b>Upload .txt/.csv file or paste Sifted URLs ‚Äî scraping starts as soon as input is given:</b>\"),\n",
        "        file_upload, url_box, run_btn, status, out\n",
        "    ])\n",
        "    display(ui)\n",
        "\n",
        "run_interface()\n"
      ]
    }
  ]
}